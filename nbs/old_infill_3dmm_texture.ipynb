{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/local/home/aarslan/miniconda3/envs/score-face/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2022-07-16 07:36:21.052860: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.\n",
            "Traceback (most recent call last):\n",
            "  File \"/local/home/aarslan/miniconda3/envs/score-face/lib/python3.9/site-packages/iopath/common/file_io.py\", line 946, in __log_tmetry_keys\n",
            "    handler.log_event()\n",
            "  File \"/local/home/aarslan/miniconda3/envs/score-face/lib/python3.9/site-packages/iopath/common/event_logger.py\", line 97, in log_event\n",
            "    del self._evt\n",
            "AttributeError: _evt\n",
            "/local/home/aarslan/miniconda3/envs/score-face/lib/python3.9/site-packages/pytorch3d/io/utils.py:66: UserWarning: Faces have invalid indices\n",
            "  warnings.warn(\"Faces have invalid indices\")\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n",
            "100%|██████████| 1000/1000 [11:56<00:00,  1.40it/s]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\"../src\")\n",
        "\n",
        "from renderer import Renderer\n",
        "from utils import get_experiment_name, save_cfg, set_seeds\n",
        "from model_utils import get_grad_texture\n",
        "from visualization import save_images, save_gif\n",
        "\n",
        "import functools\n",
        "import itertools\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "from score_sde.losses import get_optimizer\n",
        "from score_sde.models.ema import ExponentialMovingAverage\n",
        "from score_sde.utils import restore_checkpoint\n",
        "from score_sde.models import utils as mutils\n",
        "from score_sde.models import ncsnpp\n",
        "from score_sde.sde_lib import VESDE\n",
        "from score_sde.configs.ve import ffhq_256_ncsnpp_continuous as configs\n",
        "\n",
        "cfg = {\n",
        "    \"seed\": 0,\n",
        "    \"batch_size\": 1,\n",
        "    \"image_size\": 256,\n",
        "    \"texture_size\": 2048,\n",
        "    \"saved_image_size\": 256,\n",
        "    \"num_channels\": 3,\n",
        "    \"image_save_dir\": \"../results\",\n",
        "    \"device\": torch.device(\"cuda:0\"),\n",
        "    \"num_corrector_steps\": 6,\n",
        "    \"sde_N\": 1000,\n",
        "    \"snr\": 0.02, # 0.035 large\n",
        "    \"num_optimization_steps_for_filled_texture\": 350, # 400 large\n",
        "    # \"initial_noise_coefficient\": 50.0\n",
        "}\n",
        "cfg[\"experiment_name\"] = get_experiment_name()\n",
        "# cfg[\"texture_path\"] = \"/local/home/aarslan/DECA/TestSamples/examples/results/33673/33673.png\"\n",
        "# cfg[\"obj_path\"] = \"/local/home/aarslan/DECA/TestSamples/examples/results/33673/33673.obj\"\n",
        "cfg[\"texture_path\"] = \"/local/home/aarslan/score-face/assets/FFHQ_results_2048_2020/33673.png\"\n",
        "cfg[\"obj_path\"] = \"/local/home/aarslan/score-face/assets/FFHQ_results_2048_2020/33673.obj\"\n",
        "\n",
        "set_seeds(cfg)\n",
        "save_cfg(cfg)\n",
        "\n",
        "ckpt_filename = \"../assets/checkpoint_48.pth\"\n",
        "config = configs.get_config()\n",
        "sde = VESDE(sigma_min=config.model.sigma_min, sigma_max=config.model.sigma_max, N=cfg[\"sde_N\"])\n",
        "sampling_eps = 1e-5\n",
        "\n",
        "batch_size = cfg[\"batch_size\"]\n",
        "config.training.batch_size = batch_size\n",
        "config.eval.batch_size = batch_size\n",
        "config.device = cfg[\"device\"]\n",
        "\n",
        "sigmas = mutils.get_sigmas(config)\n",
        "score_model = mutils.create_model(config)\n",
        "score_model.eval()\n",
        "\n",
        "optimizer = get_optimizer(config, score_model.parameters())\n",
        "ema = ExponentialMovingAverage(score_model.parameters(),\n",
        "                               decay=config.model.ema_rate)\n",
        "state = dict(step=0, optimizer=optimizer, model=score_model, ema=ema)\n",
        "state = restore_checkpoint(ckpt_filename, state, config.device)\n",
        "ema.copy_to(score_model.parameters())\n",
        "\n",
        "def render(texture, background, pixel_uvs, background_mask):\n",
        "    texture_sampled = torch.nn.functional.grid_sample(torch.flip(texture, dims=[-2]), pixel_uvs, mode='bilinear', padding_mode='border', align_corners=False)\n",
        "    return torch.where(background_mask, background, texture_sampled)\n",
        "\n",
        "image_shape = (cfg[\"batch_size\"], cfg[\"num_channels\"], cfg[\"image_size\"], cfg[\"image_size\"])\n",
        "texture_shape = (cfg[\"batch_size\"], cfg[\"num_channels\"], cfg[\"texture_size\"], cfg[\"texture_size\"])\n",
        "\n",
        "def get_score_fn(sde, score_model):\n",
        "    def score_fn(x, vec_t):\n",
        "        labels = sde.marginal_prob(torch.zeros_like(x), vec_t)[1]\n",
        "        score = score_model(x, labels)\n",
        "        return score\n",
        "    return score_fn\n",
        "\n",
        "probability_flow = False\n",
        "score_fn = get_score_fn(sde, score_model)\n",
        "rsde = sde.reverse(score_fn=score_fn, probability_flow=probability_flow)\n",
        "timesteps = torch.linspace(sde.T, sampling_eps, sde.N)\n",
        "zeros = torch.zeros(size=image_shape, device=cfg[\"device\"])\n",
        "\n",
        "flame_texture = cv2.imread(cfg[\"texture_path\"])[:, :, [2, 1, 0]]\n",
        "flame_texture = cv2.resize(flame_texture, dsize=(cfg[\"texture_size\"], cfg[\"texture_size\"]))\n",
        "flame_texture = torch.tensor(flame_texture, device=cfg[\"device\"]).permute(2, 0, 1).unsqueeze(0) / 255.0  # (N, C, H, W)\n",
        "filled_mask = flame_texture.sum(axis=1).bool().repeat(repeats=[1, 3, 1, 1])\n",
        "\n",
        "prerender_results = {}\n",
        "elevs = [0.0]\n",
        "azimuths = [-35.0]\n",
        "renderer = Renderer(cfg=cfg)\n",
        "\n",
        "for elev, azimuth in tqdm(list(itertools.product(elevs, azimuths))):\n",
        "    pixel_uvs, background_mask = renderer.render(texture=flame_texture, background=zeros, elev=elev, azimuth=azimuth, result_keys=[\"pixel_uvs\", \"background_mask\"])\n",
        "    prerender_results[f\"{elev}_{azimuth}\"] = {\"pixel_uvs\": pixel_uvs,\n",
        "                                              \"background_mask\": background_mask}\n",
        "\n",
        "pixel_uvs = torch.tensor(prerender_results[\"0.0_-35.0\"][\"pixel_uvs\"], device=cfg[\"device\"])\n",
        "background_mask = torch.tensor(prerender_results[\"0.0_-35.0\"][\"background_mask\"], device=cfg[\"device\"]).bool()\n",
        "\n",
        "demo_face = render(texture=flame_texture, background=zeros, pixel_uvs=pixel_uvs, background_mask=background_mask)\n",
        "save_images(cfg=cfg, _images=demo_face, image_type=\"demo_face\", iteration=0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(list(range(sde.N))):\n",
        "        t = timesteps[i]\n",
        "        vec_t = torch.ones(cfg[\"batch_size\"], device=cfg[\"device\"]) * t\n",
        "        alpha = torch.ones_like(vec_t)\n",
        "        \n",
        "        background_mean, std = sde.marginal_prob(zeros, vec_t)\n",
        "        background = background_mean + torch.randn(size=image_shape, device=cfg[\"device\"]) * std[:, None, None, None]\n",
        "\n",
        "        random_texture = torch.randn(size=texture_shape, device=cfg[\"device\"]) * std[:, None, None, None] # * cfg[\"initial_noise_coefficient\"] / 348.0\n",
        "\n",
        "        if i == 0:    \n",
        "            unfilled_texture = random_texture * (1 - 1 * filled_mask)\n",
        "        \n",
        "        if i <= sde.N - cfg[\"num_optimization_steps_for_filled_texture\"]:\n",
        "            filled_texture = (flame_texture + random_texture) * (1 * filled_mask)\n",
        "\n",
        "        for j in range(cfg[\"num_corrector_steps\"]):\n",
        "            texture = torch.where(filled_mask, filled_texture, unfilled_texture)\n",
        "            face = render(texture=texture, background=background, pixel_uvs=pixel_uvs, background_mask=background_mask)\n",
        "            grad_face = score_fn(face, vec_t)\n",
        "            render_func = functools.partial(render, background=background, pixel_uvs=pixel_uvs, background_mask=background_mask)\n",
        "            grad_texture = get_grad_texture(texture=texture, grad_face=grad_face, render_func=render_func)\n",
        "            grad_filled_texture = grad_texture * filled_mask\n",
        "            grad_unfilled_texture = grad_texture * (1 - 1 * filled_mask)\n",
        "\n",
        "            grad_texture_norm = torch.norm(grad_texture.reshape(grad_texture.shape[0], -1), dim=-1).mean()\n",
        "\n",
        "            if j == cfg[\"num_corrector_steps\"] - 1:\n",
        "                grad_texture_sum = grad_texture.sum(axis=1)\n",
        "                grad_texture_zero_one = 1 * torch.logical_and(grad_texture_sum, torch.ones_like(grad_texture_sum)).unsqueeze(0).repeat(repeats=[1, 3, 1, 1])\n",
        "\n",
        "            noise_texture = torch.randn_like(texture)\n",
        "            noise_texture_norm = torch.norm(noise_texture.reshape(noise_texture.shape[0], -1), dim=-1).mean()\n",
        "            step_size_texture = (cfg[\"snr\"] * noise_texture_norm / grad_texture_norm) ** 2 * 2 * alpha\n",
        "\n",
        "            if i >= sde.N - cfg[\"num_optimization_steps_for_filled_texture\"]:\n",
        "                filled_texture_mean = filled_texture + step_size_texture[:, None, None, None] * grad_filled_texture\n",
        "                filled_texture = filled_texture_mean + torch.sqrt(step_size_texture * 2)[:, None, None, None] * noise_texture\n",
        "            \n",
        "            unfilled_texture_mean = unfilled_texture + step_size_texture[:, None, None, None] * grad_unfilled_texture\n",
        "            unfilled_texture = unfilled_texture_mean + torch.sqrt(step_size_texture * 2)[:, None, None, None] * noise_texture\n",
        "\n",
        "        if i == 0:\n",
        "            save_images(cfg, _images=grad_texture_zero_one, image_type=\"texture_gradient\", iteration=i)\n",
        "            save_images(cfg, _images=(1 * background_mask), image_type=\"background_mask\", iteration=i)\n",
        "        if i % 10 == 0:\n",
        "            save_images(cfg=cfg, _images=face, image_type=\"face\", iteration=i)\n",
        "            save_images(cfg=cfg, _images=texture, image_type=\"texture\", iteration=i)\n",
        "            save_images(cfg=cfg, _images=background, image_type=\"background\", iteration=i)\n",
        "        if i == sde.N - 1:\n",
        "            save_gif(cfg=cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/local/home/aarslan/miniconda3/envs/score-face/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2022-07-14 15:43:09.207183: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.\n",
            "Traceback (most recent call last):\n",
            "  File \"/local/home/aarslan/miniconda3/envs/score-face/lib/python3.9/site-packages/iopath/common/file_io.py\", line 946, in __log_tmetry_keys\n",
            "    handler.log_event()\n",
            "  File \"/local/home/aarslan/miniconda3/envs/score-face/lib/python3.9/site-packages/iopath/common/event_logger.py\", line 97, in log_event\n",
            "    del self._evt\n",
            "AttributeError: _evt\n",
            "/local/home/aarslan/miniconda3/envs/score-face/lib/python3.9/site-packages/pytorch3d/io/utils.py:66: UserWarning: Faces have invalid indices\n",
            "  warnings.warn(\"Faces have invalid indices\")\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(50., device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [14:21<00:00,  1.16it/s]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\"../src\")\n",
        "import itertools\n",
        "from renderer import Renderer\n",
        "from utils import get_experiment_name, save_cfg, set_seeds\n",
        "from model_utils import get_grad_texture, get_grad_texture_and_background\n",
        "from visualization import save_images, save_gif\n",
        "import torch\n",
        "import functools\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from score_sde.losses import get_optimizer\n",
        "from score_sde.models.ema import ExponentialMovingAverage\n",
        "from score_sde.utils import restore_checkpoint\n",
        "from score_sde.models import utils as mutils\n",
        "from score_sde.models import ncsnpp\n",
        "from score_sde.sde_lib import VESDE\n",
        "from score_sde.configs.ve import ffhq_256_ncsnpp_continuous as configs\n",
        "\n",
        "\n",
        "cfg = {\n",
        "    \"seed\": 0,\n",
        "    \"batch_size\": 1,\n",
        "    \"image_size\": 256,\n",
        "    \"texture_size\": 2048,\n",
        "    \"saved_image_size\": 256,\n",
        "    \"num_channels\": 3,\n",
        "    \"image_save_dir\": \"../results\",\n",
        "    \"device\": torch.device(\"cuda:0\"),\n",
        "    \"num_corrector_steps\": 6,\n",
        "    \"sde_N\": 1000,\n",
        "    \"snr\": 0.015, # tune\n",
        "    \"langevin_noise_coefficient\": 1,\n",
        "    \"initial_noise_coefficient\": 50.0,\n",
        "    \"num_optimization_steps_for_filled_part\": 1000\n",
        "}\n",
        "cfg[\"obj_path\"] = f\"../assets/FFHQ_results_{cfg['texture_size']}_2020/40044.obj\"\n",
        "cfg[\"texture_path\"] = f\"../assets/FFHQ_results_{cfg['texture_size']}_2020/40044.png\"\n",
        "cfg[\"experiment_name\"] = get_experiment_name()\n",
        "set_seeds(cfg)\n",
        "save_cfg(cfg)\n",
        "\n",
        "ckpt_filename = \"../assets/checkpoint_48.pth\"\n",
        "config = configs.get_config()\n",
        "sde = VESDE(sigma_min=config.model.sigma_min, sigma_max=config.model.sigma_max, N=cfg[\"sde_N\"])\n",
        "sampling_eps = 1e-5\n",
        "\n",
        "batch_size = cfg[\"batch_size\"]\n",
        "config.training.batch_size = batch_size\n",
        "config.eval.batch_size = batch_size\n",
        "config.device = cfg[\"device\"]\n",
        "\n",
        "sigmas = mutils.get_sigmas(config)\n",
        "score_model = mutils.create_model(config)\n",
        "score_model.eval()\n",
        "\n",
        "optimizer = get_optimizer(config, score_model.parameters())\n",
        "ema = ExponentialMovingAverage(score_model.parameters(),\n",
        "                               decay=config.model.ema_rate)\n",
        "state = dict(step=0, optimizer=optimizer, model=score_model, ema=ema)\n",
        "state = restore_checkpoint(ckpt_filename, state, config.device)\n",
        "ema.copy_to(score_model.parameters())\n",
        "\n",
        "def render(texture, background, pixel_uvs, background_mask):\n",
        "    texture_sampled = torch.nn.functional.grid_sample(torch.flip(texture, dims=[-2]), pixel_uvs, mode='bilinear', padding_mode='border', align_corners=False)\n",
        "    return torch.where(background_mask, background, texture_sampled)\n",
        "\n",
        "image_shape = (cfg[\"batch_size\"], cfg[\"num_channels\"], cfg[\"image_size\"], cfg[\"image_size\"])\n",
        "texture_shape = (cfg[\"batch_size\"], cfg[\"num_channels\"], cfg[\"texture_size\"], cfg[\"texture_size\"])\n",
        "\n",
        "prerender_results = {}\n",
        "elevs = [0.0]\n",
        "azimuths = [0.0]\n",
        "renderer = Renderer(cfg=cfg)\n",
        "\n",
        "background = torch.zeros(size=image_shape, device=cfg[\"device\"])\n",
        "\n",
        "# texture which was filled while azimuth=0\n",
        "initial_texture = torch.tensor(cv2.imread(cfg[\"texture_path\"])[:, :, [2, 1, 0]], device=cfg[\"device\"]).permute(2, 0, 1).unsqueeze(0) / 255.0  # (N, C, H, W)\n",
        "\n",
        "for elev, azimuth in tqdm(list(itertools.product(elevs, azimuths))):\n",
        "    pixel_uvs, background_mask = renderer.render(texture=initial_texture, background=background, elev=elev, azimuth=azimuth, result_keys=[\"pixel_uvs\", \"background_mask\"])\n",
        "    prerender_results[f\"{elev}_{azimuth}\"] = {\"pixel_uvs\": pixel_uvs,\n",
        "                                              \"background_mask\": background_mask}\n",
        "\n",
        "pixel_uvs = torch.tensor(prerender_results[\"0.0_0.0\"][\"pixel_uvs\"], device=cfg[\"device\"])\n",
        "background_mask = torch.tensor(prerender_results[\"0.0_0.0\"][\"background_mask\"], device=cfg[\"device\"]).bool()\n",
        "\n",
        "initial_texture = torch.tensor(cv2.imread(cfg[\"texture_path\"])[:, :, [2, 1, 0]], device=cfg[\"device\"]).permute(2, 0, 1).unsqueeze(0) / 255.0  # (N, C, H, W)\n",
        "filled_mask = initial_texture.sum(axis=1, keepdim=True).bool().repeat(repeats=[1, 3, 1, 1])\n",
        "texture_half_size = (cfg[\"batch_size\"], cfg[\"num_channels\"], cfg[\"texture_size\"], int(cfg[\"texture_size\"] / 2))\n",
        "filled_mask = torch.logical_and(filled_mask, torch.concat(tensors=(torch.ones(size=texture_half_size, device=cfg[\"device\"]), torch.zeros(size=texture_half_size, device=cfg[\"device\"])), dim=-1))\n",
        "initial_texture_filled = initial_texture * filled_mask\n",
        "# current_texture_filled = deepcopy(initial_texture_filled)\n",
        "# initial_texture_unfilled = torch.randn_like(initial_texture) * (1 - 1 * filled_mask) * cfg[\"initial_noise_coefficient\"]\n",
        "# current_texture_unfilled = deepcopy(initial_texture_unfilled)\n",
        "# current_texture = torch.where(filled_mask, current_texture_filled, current_texture_unfilled)\n",
        "# initial_face = render(texture=current_texture, background=background, pixel_uvs=pixel_uvs, background_mask=background_mask)\n",
        "# save_images(cfg=cfg, _images=initial_face, image_type=\"face\", iteration=-1)\n",
        "\n",
        "def get_score_fn(sde, score_model):\n",
        "    def score_fn(x, vec_t):\n",
        "        labels = sde.marginal_prob(torch.zeros_like(x), vec_t)[1]\n",
        "        score = score_model(x, labels)\n",
        "        return score\n",
        "    return score_fn\n",
        "\n",
        "probability_flow = False\n",
        "score_fn = get_score_fn(sde, score_model)\n",
        "rsde = sde.reverse(score_fn=score_fn, probability_flow=probability_flow)\n",
        "timesteps = torch.linspace(sde.T, sampling_eps, sde.N)\n",
        "zeros = torch.zeros(size=image_shape, device=cfg[\"device\"])\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(list(range(sde.N))):\n",
        "        t = timesteps[i]\n",
        "        vec_t = torch.ones(cfg[\"batch_size\"], device=cfg[\"device\"]) * t\n",
        "        alpha = torch.ones_like(vec_t)\n",
        "        \n",
        "        _, std = sde.marginal_prob(zeros, vec_t)\n",
        "        std = std * 5 / 34.8\n",
        "        background = torch.randn(size=image_shape, device=cfg[\"device\"]) * std[:, None, None, None]\n",
        "\n",
        "        random_tensor_for_texture = torch.randn(size=texture_shape, device=cfg[\"device\"]) * std[:, None, None, None]\n",
        "\n",
        "        if i == 0:\n",
        "            print(std.mean())\n",
        "            current_texture_unfilled = random_tensor_for_texture\n",
        "\n",
        "        if i <= sde.N - cfg[\"num_optimization_steps_for_filled_part\"]:\n",
        "            # current_texture_filled = initial_texture_filled + torch.randn(size=texture_shape, device=cfg[\"device\"]) * std[:, None, None, None]\n",
        "            current_texture_filled = random_tensor_for_texture\n",
        "        \n",
        "        current_texture = torch.where(filled_mask, current_texture_filled, current_texture_unfilled)\n",
        "\n",
        "        # correct\n",
        "        for j in range(cfg[\"num_corrector_steps\"]):\n",
        "            face = render(texture=current_texture, background=background, pixel_uvs=pixel_uvs, background_mask=background_mask)\n",
        "\n",
        "            if i % 10 == 0 and j == 0:\n",
        "                save_images(cfg=cfg, _images=face, image_type=\"face\", iteration=i)\n",
        "                save_images(cfg=cfg, _images=current_texture, image_type=\"texture\", iteration=i)\n",
        "                save_images(cfg=cfg, _images=background, image_type=\"background\", iteration=i)\n",
        "\n",
        "            grad_face = score_fn(face, vec_t)\n",
        "            render_func = functools.partial(render, background=background, pixel_uvs=pixel_uvs, background_mask=background_mask)\n",
        "            grad_texture = get_grad_texture(texture=current_texture, grad_face=grad_face, render_func=render_func)\n",
        "            grad_texture_norm = torch.norm(grad_texture.reshape(grad_texture.shape[0], -1), dim=-1).mean()\n",
        "            grad_texture_filled = grad_texture * (1 * filled_mask)\n",
        "            grad_texture_unfilled = grad_texture * (1 - 1 * filled_mask)\n",
        "\n",
        "            noise_texture = torch.randn_like(current_texture)\n",
        "            noise_texture_norm = torch.norm(noise_texture.reshape(noise_texture.shape[0], -1), dim=-1).mean()\n",
        "            step_size_texture = (cfg[\"snr\"] * noise_texture_norm / grad_texture_norm) ** 2 * 2 * alpha\n",
        "\n",
        "            if j == cfg[\"num_corrector_steps\"] - 1:\n",
        "                grad_texture_unfilled_sum = grad_texture_unfilled.sum(axis=1)\n",
        "                grad_texture_unfilled_zero_one = 1 * grad_texture_unfilled_sum.bool().unsqueeze(0).repeat(repeats=[1, 3, 1, 1])\n",
        "\n",
        "                grad_texture_filled_sum = grad_texture_filled.sum(axis=1)\n",
        "                grad_texture_filled_zero_one = 1 * grad_texture_filled_sum.bool().unsqueeze(0).repeat(repeats=[1, 3, 1, 1])\n",
        "\n",
        "            current_texture_unfilled_mean = current_texture_unfilled + step_size_texture[:, None, None, None] * grad_texture_unfilled\n",
        "            current_texture_unfilled = current_texture_unfilled_mean + torch.sqrt(step_size_texture * 2)[:, None, None, None] * noise_texture * cfg[\"langevin_noise_coefficient\"]\n",
        "\n",
        "            if i >= sde.N - cfg[\"num_optimization_steps_for_filled_part\"]:\n",
        "                current_texture_filled_mean = current_texture_filled + step_size_texture[:, None, None, None] * grad_texture_filled\n",
        "                current_texture_filled = current_texture_filled_mean + torch.sqrt(step_size_texture * 2)[:, None, None, None] * noise_texture * cfg[\"langevin_noise_coefficient\"]\n",
        "\n",
        "        if i == 0:\n",
        "            save_images(cfg, _images=grad_texture_unfilled_zero_one, image_type=\"texture_unfilled_gradient\", iteration=i)\n",
        "            save_images(cfg, _images=(1 * background_mask), image_type=\"background_mask\", iteration=i)\n",
        "\n",
        "        if i == sde.N - 1:\n",
        "            save_gif(cfg=cfg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa9OIcJmUKmZ",
        "outputId": "8fde468b-2c95-4003-f0bd-20ddad5248e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/local/home/aarslan/miniconda3/envs/score-face/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2022-07-12 17:41:02.368124: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.\n",
            "Traceback (most recent call last):\n",
            "  File \"/local/home/aarslan/miniconda3/envs/score-face/lib/python3.9/site-packages/iopath/common/file_io.py\", line 946, in __log_tmetry_keys\n",
            "    handler.log_event()\n",
            "  File \"/local/home/aarslan/miniconda3/envs/score-face/lib/python3.9/site-packages/iopath/common/event_logger.py\", line 97, in log_event\n",
            "    del self._evt\n",
            "AttributeError: _evt\n",
            "/local/home/aarslan/miniconda3/envs/score-face/lib/python3.9/site-packages/pytorch3d/io/utils.py:66: UserWarning: Faces have invalid indices\n",
            "  warnings.warn(\"Faces have invalid indices\")\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
            " 41%|████      | 412/1000 [06:41<09:32,  1.03it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/local/home/aarslan/score-face/nbs/main.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 128>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bait-server-02.ethz.ch/local/home/aarslan/score-face/nbs/main.ipynb#ch0000000vscode-remote?line=168'>169</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(cfg[\u001b[39m\"\u001b[39m\u001b[39mnum_corrector_steps\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bait-server-02.ethz.ch/local/home/aarslan/score-face/nbs/main.ipynb#ch0000000vscode-remote?line=169'>170</a>\u001b[0m     face \u001b[39m=\u001b[39m render(texture\u001b[39m=\u001b[39mtexture, background\u001b[39m=\u001b[39mbackground, pixel_uvs\u001b[39m=\u001b[39mpixel_uvs, background_mask\u001b[39m=\u001b[39mbackground_mask)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bait-server-02.ethz.ch/local/home/aarslan/score-face/nbs/main.ipynb#ch0000000vscode-remote?line=170'>171</a>\u001b[0m     grad_face \u001b[39m=\u001b[39m score_fn(face, vec_t)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bait-server-02.ethz.ch/local/home/aarslan/score-face/nbs/main.ipynb#ch0000000vscode-remote?line=171'>172</a>\u001b[0m     grad_face_norm \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnorm(grad_face\u001b[39m.\u001b[39mreshape(grad_face\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bait-server-02.ethz.ch/local/home/aarslan/score-face/nbs/main.ipynb#ch0000000vscode-remote?line=172'>173</a>\u001b[0m     \u001b[39mif\u001b[39;00m cfg[\u001b[39m\"\u001b[39m\u001b[39mbackground\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moptimize\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "\u001b[1;32m/local/home/aarslan/score-face/nbs/main.ipynb Cell 1\u001b[0m in \u001b[0;36mget_score_fn.<locals>.score_fn\u001b[0;34m(x, vec_t)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bait-server-02.ethz.ch/local/home/aarslan/score-face/nbs/main.ipynb#ch0000000vscode-remote?line=88'>89</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscore_fn\u001b[39m(x, vec_t):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bait-server-02.ethz.ch/local/home/aarslan/score-face/nbs/main.ipynb#ch0000000vscode-remote?line=89'>90</a>\u001b[0m     labels \u001b[39m=\u001b[39m sde\u001b[39m.\u001b[39mmarginal_prob(torch\u001b[39m.\u001b[39mzeros_like(x), vec_t)[\u001b[39m1\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bait-server-02.ethz.ch/local/home/aarslan/score-face/nbs/main.ipynb#ch0000000vscode-remote?line=90'>91</a>\u001b[0m     score \u001b[39m=\u001b[39m score_model(x, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bait-server-02.ethz.ch/local/home/aarslan/score-face/nbs/main.ipynb#ch0000000vscode-remote?line=91'>92</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m score\n",
            "File \u001b[0;32m~/miniconda3/envs/score-face/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda3/envs/score-face/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:168\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[1;32m    167\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 168\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
            "File \u001b[0;32m~/miniconda3/envs/score-face/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:178\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_apply\u001b[39m(\u001b[39mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 178\u001b[0m     \u001b[39mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(replicas)])\n",
            "File \u001b[0;32m~/miniconda3/envs/score-face/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py:80\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     78\u001b[0m         thread\u001b[39m.\u001b[39mjoin()\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     _worker(\u001b[39m0\u001b[39;49m, modules[\u001b[39m0\u001b[39;49m], inputs[\u001b[39m0\u001b[39;49m], kwargs_tup[\u001b[39m0\u001b[39;49m], devices[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     82\u001b[0m outputs \u001b[39m=\u001b[39m []\n\u001b[1;32m     83\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(inputs)):\n",
            "File \u001b[0;32m~/miniconda3/envs/score-face/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py:61\u001b[0m, in \u001b[0;36mparallel_apply.<locals>._worker\u001b[0;34m(i, module, input, kwargs, device)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39minput\u001b[39m, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m     60\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m (\u001b[39minput\u001b[39m,)\n\u001b[0;32m---> 61\u001b[0m     output \u001b[39m=\u001b[39m module(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     62\u001b[0m \u001b[39mwith\u001b[39;00m lock:\n\u001b[1;32m     63\u001b[0m     results[i] \u001b[39m=\u001b[39m output\n",
            "File \u001b[0;32m~/miniconda3/envs/score-face/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/score-face/nbs/../src/score_sde/models/ncsnpp.py:286\u001b[0m, in \u001b[0;36mNCSNpp.forward\u001b[0;34m(self, x, time_cond)\u001b[0m\n\u001b[1;32m    284\u001b[0m   m_idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    285\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m   h \u001b[39m=\u001b[39m modules[m_idx](hs[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], temb)\n\u001b[1;32m    287\u001b[0m   m_idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    289\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogressive_input \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39minput_skip\u001b[39m\u001b[39m'\u001b[39m:\n",
            "File \u001b[0;32m~/miniconda3/envs/score-face/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/score-face/nbs/../src/score_sde/models/layerspp.py:263\u001b[0m, in \u001b[0;36mResnetBlockBigGANpp.forward\u001b[0;34m(self, x, temb)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39m# Add bias to each feature map conditioned on the time embedding\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[39mif\u001b[39;00m temb \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 263\u001b[0m   h \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mDense_0(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(temb))[:, :, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m    264\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mGroupNorm_1(h))\n\u001b[1;32m    265\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDropout_0(h)\n",
            "File \u001b[0;32m~/miniconda3/envs/score-face/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda3/envs/score-face/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\"../src\")\n",
        "import itertools\n",
        "from renderer import Renderer\n",
        "from utils import get_experiment_name, save_cfg, set_seeds\n",
        "from model_utils import get_grad_texture, get_grad_texture_and_background\n",
        "from visualization import save_images, save_gif\n",
        "import torch\n",
        "import numpy as np\n",
        "import functools\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from score_sde.losses import get_optimizer\n",
        "from score_sde.models.ema import ExponentialMovingAverage\n",
        "from score_sde.utils import restore_checkpoint\n",
        "from score_sde.models import utils as mutils\n",
        "from score_sde.models import ncsnpp\n",
        "from score_sde.sde_lib import VESDE\n",
        "from score_sde.configs.ve import ffhq_256_ncsnpp_continuous as configs\n",
        "\n",
        "cfg = {\n",
        "    \"seed\": 0,\n",
        "    \"batch_size\": 1,\n",
        "    \"image_size\": 256,\n",
        "    \"texture_size\": 2048,\n",
        "    \"saved_image_size\": 256,\n",
        "    \"num_channels\": 3,\n",
        "    \"obj_path\": \"../assets/40044.obj\",\n",
        "    \n",
        "    \"image_save_dir\": \"../../results\",\n",
        "    \"device\": torch.device(\"cuda:0\"),\n",
        "    \"num_corrector_steps\": 6,\n",
        "    \"sde_N\": 1000,\n",
        "\n",
        "    \"texture_snr\": 0.015,\n",
        "    \"texture_initial_noise_coefficient\": 50.0,\n",
        "    \"texture_langevin_noise_coefficient\": 1,\n",
        "\n",
        "    \"background_snr\": 0.075,\n",
        "    \"background_initial_noise_coefficient\": 348.0,\n",
        "    \"background_langevin_noise_coefficient\": 1,\n",
        "\n",
        "    \"elev\": 0.0,\n",
        "    \"azimuth\": 0.0,\n",
        "\n",
        "    \"background\": \"conditional\", # fixed, optimize, conditional\n",
        "    \"texture\": \"ground_truth\", # full_noise, slightly_noise, ground_truth,\n",
        "    \"predict\": False # True, False\n",
        "}\n",
        "\n",
        "cfg[\"experiment_name\"] = get_experiment_name()\n",
        "set_seeds(cfg)\n",
        "save_cfg(cfg)\n",
        "\n",
        "ckpt_filename = \"../assets/checkpoint_48.pth\"\n",
        "config = configs.get_config()\n",
        "sde = VESDE(sigma_min=config.model.sigma_min, sigma_max=config.model.sigma_max, N=cfg[\"sde_N\"])\n",
        "sampling_eps = 1e-5\n",
        "\n",
        "batch_size = cfg[\"batch_size\"]\n",
        "config.training.batch_size = batch_size\n",
        "config.eval.batch_size = batch_size\n",
        "config.device = cfg[\"device\"]\n",
        "\n",
        "sigmas = mutils.get_sigmas(config)\n",
        "score_model = mutils.create_model(config)\n",
        "score_model.eval()\n",
        "\n",
        "optimizer = get_optimizer(config, score_model.parameters())\n",
        "ema = ExponentialMovingAverage(score_model.parameters(),\n",
        "                               decay=config.model.ema_rate)\n",
        "state = dict(step=0, optimizer=optimizer,\n",
        "             model=score_model, ema=ema)\n",
        "\n",
        "state = restore_checkpoint(ckpt_filename, state, config.device)\n",
        "ema.copy_to(score_model.parameters())\n",
        "\n",
        "def render(texture, background, pixel_uvs, background_mask):\n",
        "    texture_sampled = torch.nn.functional.grid_sample(torch.flip(texture, dims=[-2]), pixel_uvs, mode='bilinear', padding_mode='border', align_corners=False)\n",
        "    return torch.where(background_mask, background, texture_sampled)\n",
        "\n",
        "image_shape = (cfg[\"batch_size\"], cfg[\"num_channels\"], cfg[\"image_size\"], cfg[\"image_size\"])\n",
        "texture_shape = (cfg[\"batch_size\"], cfg[\"num_channels\"], cfg[\"texture_size\"], cfg[\"texture_size\"])\n",
        "\n",
        "def get_score_fn(sde, score_model):\n",
        "    def score_fn(x, vec_t):\n",
        "        labels = sde.marginal_prob(torch.zeros_like(x), vec_t)[1]\n",
        "        score = score_model(x, labels)\n",
        "        return score\n",
        "    return score_fn\n",
        "\n",
        "probability_flow = False\n",
        "score_fn = get_score_fn(sde, score_model)\n",
        "rsde = sde.reverse(score_fn=score_fn, probability_flow=probability_flow)\n",
        "timesteps = torch.linspace(sde.T, sampling_eps, sde.N)\n",
        "zeros = torch.zeros(size=image_shape, device=cfg[\"device\"])\n",
        "\n",
        "if cfg[\"background\"] in [\"conditional\", \"optimize\"]:\n",
        "    background = torch.randn(size=image_shape, device=cfg[\"device\"]) * cfg[\"background_initial_noise_coefficient\"]\n",
        "else:\n",
        "    background = torch.zeros(size=image_shape, device=cfg[\"device\"])\n",
        "\n",
        "if cfg[\"texture\"] == \"full_noise\":\n",
        "    texture = torch.randn(size=texture_shape, device=cfg[\"device\"]) * cfg[\"texture_initial_noise_coefficient\"]\n",
        "elif cfg[\"texture\"] == \"slightly_noise\":\n",
        "    texture = torch.tensor(cv2.imread(\"../assets/texture.png\")[:, :, [2, 1, 0]], device=cfg[\"device\"]).permute(2, 0, 1).unsqueeze(0) / 255.0 + 0.75 * torch.randn(size=texture_shape, device=cfg[\"device\"])\n",
        "elif cfg[\"texture\"] == \"ground_truth\":\n",
        "    \n",
        "\n",
        "prerender_results = {}\n",
        "elevs = [0.0]\n",
        "azimuths = [0.0]\n",
        "renderer = Renderer(cfg=cfg)\n",
        "\n",
        "for elev, azimuth in tqdm(list(itertools.product(elevs, azimuths))):\n",
        "    pixel_uvs, background_mask = renderer.render(texture=texture, background=background, elev=elev, azimuth=azimuth, result_keys=[\"pixel_uvs\", \"background_mask\"])\n",
        "    prerender_results[f\"{elev}_{azimuth}\"] = {\"pixel_uvs\": pixel_uvs,\n",
        "                                              \"background_mask\": background_mask}\n",
        "\n",
        "pixel_uvs = torch.tensor(prerender_results[\"0.0_0.0\"][\"pixel_uvs\"], device=cfg[\"device\"])\n",
        "background_mask = torch.tensor(prerender_results[\"0.0_0.0\"][\"background_mask\"], device=cfg[\"device\"]).bool()\n",
        "\n",
        "face = render(texture=texture, background=background, pixel_uvs=pixel_uvs, background_mask=background_mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(list(range(sde.N))):\n",
        "        if i == 1:\n",
        "            save_images(cfg, _images=grad_texture_zero_one, image_type=\"texture_gradient\", iteration=i)\n",
        "            save_images(cfg, _images=(1 * background_mask), image_type=\"background_mask\", iteration=i)\n",
        "        if (i + 1) % 10 == 0:\n",
        "            save_images(cfg=cfg, _images=face, image_type=\"face\", iteration=i)\n",
        "            save_images(cfg=cfg, _images=texture, image_type=\"texture\", iteration=i)\n",
        "            save_images(cfg=cfg, _images=background, image_type=\"background\", iteration=i)\n",
        "        if i == sde.N - 1:\n",
        "            save_gif(cfg=cfg)\n",
        "\n",
        "        t = timesteps[i]\n",
        "        vec_t = torch.ones(cfg[\"batch_size\"], device=cfg[\"device\"]) * t\n",
        "        alpha = torch.ones_like(vec_t)\n",
        "        \n",
        "        if cfg[\"background\"] == \"conditional\":\n",
        "            background_mean, std = sde.marginal_prob(zeros, vec_t)\n",
        "            background = background_mean + torch.randn(size=image_shape, device=cfg[\"device\"]) * std[:, None, None, None]\n",
        "\n",
        "        if cfg[\"predict\"]:\n",
        "            f, G = rsde.discretize(face, vec_t)\n",
        "            z = torch.randn_like(face)\n",
        "            face_mean = face - f\n",
        "            face = face_mean + G[:, None, None, None] * z\n",
        "\n",
        "            if cfg[\"background\"] == \"optimize\":\n",
        "                background_mean = background - f\n",
        "                background = background_mean + G[:, None, None, None] * z\n",
        "            elif cfg[\"background\"] in [\"conditional\", \"fixed\"]:\n",
        "                pass\n",
        "\n",
        "        # correct\n",
        "        for j in range(cfg[\"num_corrector_steps\"]):\n",
        "            face = render(texture=texture, background=background, pixel_uvs=pixel_uvs, background_mask=background_mask)\n",
        "            grad_face = score_fn(face, vec_t)\n",
        "            grad_face_norm = torch.norm(grad_face.reshape(grad_face.shape[0], -1), dim=-1).mean()\n",
        "            if cfg[\"background\"] == \"optimize\":\n",
        "                render_func = functools.partial(render, pixel_uvs=pixel_uvs, background_mask=background_mask)\n",
        "                grad_texture, grad_background = get_grad_texture_and_background(texture=texture, background=background, grad_face=grad_face, render_func=render_func)\n",
        "            else:\n",
        "                render_func = functools.partial(render, background=background, pixel_uvs=pixel_uvs, background_mask=background_mask)\n",
        "                grad_texture = get_grad_texture(texture=texture, grad_face=grad_face, render_func=render_func)\n",
        "\n",
        "            if j == cfg[\"num_corrector_steps\"] - 1:\n",
        "                grad_texture_sum = grad_texture.sum(axis=1)\n",
        "                grad_texture_zero_one = 1 * torch.logical_and(grad_texture_sum, torch.ones_like(grad_texture_sum)).unsqueeze(0).repeat(repeats=[1, 3, 1, 1])\n",
        "                noise_texture = torch.randn_like(texture)\n",
        "                noise_texture_norm = torch.norm(noise_texture.reshape(noise_texture.shape[0], -1), dim=-1).mean()\n",
        "                grad_texture_norm = torch.norm(grad_texture.reshape(grad_texture.shape[0], -1), dim=-1).mean()\n",
        "                step_size_texture = (cfg[\"texture_snr\"] * noise_texture_norm / grad_texture_norm) ** 2 * 2 * alpha\n",
        "                texture_mean = texture + step_size_texture[:, None, None, None] * grad_texture\n",
        "                texture = texture_mean + torch.sqrt(step_size_texture * 2)[:, None, None, None] * noise_texture * cfg[\"texture_langevin_noise_coefficient\"]\n",
        "            \n",
        "            if cfg[\"background\"] == \"optimize\":\n",
        "                noise_background = torch.randn_like(background)\n",
        "                noise_background_norm = torch.norm(noise_background.reshape(noise_background.shape[0], -1), dim=-1).mean()\n",
        "                grad_background_norm = torch.norm(grad_background.reshape(grad_background.shape[0], -1), dim=-1).mean()\n",
        "                step_size_background = (cfg[\"background_snr\"] * noise_background_norm / grad_background_norm) ** 2 * 2 * alpha\n",
        "                background = background + step_size_background[:, None, None, None] * grad_background\n",
        "                background = background + torch.sqrt(step_size_background * 2)[:, None, None, None] * noise_background * cfg[\"background_langevin_noise_coefficient\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Score SDE demo PyTorch",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.9 ('score-face')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "491c9c9f23882fb46a081b2cf9604c8f9fab771937955179aea2894524e8d30e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
