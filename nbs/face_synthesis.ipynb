{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "cfg = {\n",
    "    \"texture_width\": 256,\n",
    "    \"texture_height\": 256,\n",
    "    \"texture_num_channels\": 3,\n",
    "    \"batch_size\": 1,\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device {device}.\")\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/cluster/scratch/aarslan/miniconda3/envs/score_face/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device cuda:0.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "import models\n",
    "from models import ncsnpp\n",
    "print(\"here\")\n",
    "from models import utils as mutils\n",
    "from losses import get_optimizer\n",
    "\n",
    "\n",
    "print(\"here\")\n",
    "from models.ema import ExponentialMovingAverage\n",
    "print(\"here\")\n",
    "from utils import restore_checkpoint\n",
    "print(\"here\")\n",
    "from configs.ve import ffhq_256_ncsnpp_continuous as configs\n",
    "print(\"here\")\n",
    "from sde_lib import VESDE\n",
    "print(\"here\")\n",
    "import datasets\n",
    "\n",
    "ckpt_filename = \"../exp/ve/ffhq_256_ncsnpp_continuous/checkpoint_24.pth\"\n",
    "config = configs.get_config()\n",
    "sde = VESDE(sigma_min=config.model.sigma_min, sigma_max=config.model.sigma_max, N=config.model.num_scales)\n",
    "sampling_eps = 1e-5\n",
    "batch_size = 1 #@param {\"type\":\"integer\"}\n",
    "config.training.batch_size = batch_size\n",
    "config.eval.batch_size = batch_size\n",
    "random_seed = 0 #@param {\"type\": \"integer\"}\n",
    "\n",
    "sigmas = mutils.get_sigmas(config)\n",
    "scaler = datasets.get_data_scaler(config)\n",
    "inverse_scaler = datasets.get_data_inverse_scaler(config)\n",
    "score_model = mutils.create_model(config)\n",
    "\n",
    "optimizer = get_optimizer(config, score_model.parameters())\n",
    "ema = ExponentialMovingAverage(score_model.parameters(),\n",
    "                               decay=config.model.ema_rate)\n",
    "state = dict(step=0, optimizer=optimizer,\n",
    "             model=score_model, ema=ema)\n",
    "\n",
    "state = restore_checkpoint(ckpt_filename, state, config.device)\n",
    "ema.copy_to(score_model.parameters())\n",
    "\n",
    "def image_grid(x):\n",
    "    size = config.data.image_size\n",
    "    channels = config.data.num_channels\n",
    "    img = x.reshape(-1, size, size, channels)\n",
    "    w = int(np.sqrt(img.shape[0]))\n",
    "    img = img.reshape((w, w, size, size, channels)).transpose((0, 2, 1, 3, 4)).reshape((w * size, w * size, channels))\n",
    "    return img\n",
    "\n",
    "def show_samples(x):\n",
    "    x = x.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
    "    img = image_grid(x)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "snr = 0.075 #@param {\"type\": \"number\"}\n",
    "n_steps = 1 # @param {\"type\": \"integer\"}\n",
    "probability_flow = False #@param {\"type\": \"boolean\"}"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/cluster/scratch/aarslan/miniconda3/envs/score_face/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pytorch3d.io import load_obj\n",
    "from pytorch3d.renderer.mesh import Textures\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    FoVPerspectiveCameras, \n",
    "    PointLights, \n",
    "    Materials, \n",
    "    RasterizationSettings, \n",
    "    MeshRenderer, \n",
    "    MeshRasterizer,  \n",
    "    SoftPhongShader\n",
    ")\n",
    "\n",
    "class Renderer(object):\n",
    "    def __init__(self):\n",
    "        self.raster_settings = RasterizationSettings(\n",
    "            image_size=256,\n",
    "            blur_radius=0.0,\n",
    "            bin_size=0,\n",
    "        )\n",
    "        obj_path = \"/cluster/scratch/aarslan/FFHQ/mesh_and_texture/20705.obj\"\n",
    "        self.verts, self.faces, self.aux = load_obj(obj_path, device=device)\n",
    "        self.verts_uvs = self.aux.verts_uvs[None, ...]  # (1, V, 2)\n",
    "        self.faces_uvs = self.faces.textures_idx[None, ...]  # (1, F, 3)\n",
    "\n",
    "    def render(self, texture, elev, azimuth):\n",
    "        \"\"\"\n",
    "            Inputs:\n",
    "                textures: torch.tensor with shape (N, C, H, W)\n",
    "                elev: float\n",
    "                azimuth: float\n",
    "            Outputs:\n",
    "                face images: (N, C, H, W)\n",
    "        \"\"\"\n",
    "\n",
    "        texture = texture.permute(0, 2, 3, 1) # (N, H, W, C)\n",
    "        tex = Textures(verts_uvs=self.verts_uvs, faces_uvs=self.faces_uvs, maps=texture)\n",
    "        meshes = Meshes(verts=[self.verts], faces=[self.faces.verts_idx], textures=tex)\n",
    "        verts_packed = meshes.verts_packed()\n",
    "        N = verts_packed.shape[0]\n",
    "        center = verts_packed.mean(0)\n",
    "        scale = max((verts_packed - center).abs().max(0)[0])\n",
    "        meshes.offset_verts_(-center)\n",
    "        meshes.scale_verts_((1.0 / float(scale)))\n",
    "        R, T = look_at_view_transform(2.0, elev, azimuth)\n",
    "        cameras = FoVPerspectiveCameras(R=R, T=T, znear=1, zfar=10000, fov=40, degrees=True, device=device)\n",
    "        lights = PointLights(diffuse_color=((0.7, 0.7, 0.7),), ambient_color=((0.2, 0.2, 0.2),), device=device, location=T)\n",
    "        renderer = MeshRenderer(\n",
    "            rasterizer=MeshRasterizer(\n",
    "                cameras=cameras, \n",
    "                raster_settings=self.raster_settings\n",
    "            ),\n",
    "            shader=SoftPhongShader(\n",
    "                device=device, \n",
    "                cameras=cameras,\n",
    "                lights=lights\n",
    "            )\n",
    "        )\n",
    "        face = renderer(meshes)[:, :, :, :3].permute(0, 3, 1, 2)  # (N, C, H, W)\n",
    "        return face\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# from dataclasses import dataclass, field\n",
    "# import matplotlib.pyplot as plt\n",
    "# import io\n",
    "# import csv\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib\n",
    "# import importlib\n",
    "# import os\n",
    "# import functools\n",
    "# import itertools\n",
    "# import torch\n",
    "from losses import get_optimizer\n",
    "from models.ema import ExponentialMovingAverage\n",
    "\n",
    "# import torch.nn as nn\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds\n",
    "# import tensorflow_gan as tfgan\n",
    "# import tqdm\n",
    "# import io\n",
    "# import likelihood\n",
    "# import controllable_generation\n",
    "\n",
    "# sns.set(font_scale=2)\n",
    "# sns.set(style=\"whitegrid\")\n",
    "\n",
    "import models\n",
    "from models import utils as mutils\n",
    "from models import ncsnv2\n",
    "from models import ncsnpp\n",
    "from models import ddpm as ddpm_model\n",
    "from models import layerspp\n",
    "from models import layers\n",
    "from models import normalization\n",
    "import sampling\n",
    "from likelihood import get_likelihood_fn\n",
    "from sde_lib import VESDE, VPSDE, subVPSDE\n",
    "from sampling import (ReverseDiffusionPredictor, \n",
    "                      LangevinCorrector, \n",
    "                      EulerMaruyamaPredictor, \n",
    "                      AncestralSamplingPredictor, \n",
    "                      NoneCorrector, \n",
    "                      NonePredictor,\n",
    "                      AnnealedLangevinDynamics)\n",
    "import datasets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\n",
    "\n",
    "\n",
    "def image_grid(x):\n",
    "    size = config.data.image_size\n",
    "    channels = config.data.num_channels\n",
    "    img = x.reshape(-1, size, size, channels)\n",
    "    w = int(np.sqrt(img.shape[0]))\n",
    "    img = img.reshape((w, w, size, size, channels)).transpose((0, 2, 1, 3, 4)).reshape((w * size, w * size, channels))\n",
    "    return img\n",
    "\n",
    "\n",
    "def show_samples(x):\n",
    "    x = x.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
    "    img = image_grid(x)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2022-05-26 09:34:50.332055: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "imported from datasets\n",
      "imported from losses\n",
      "imported from sde_lib\n",
      "imported from models\n",
      "imported from models\n",
      "imported from utils\n",
      "imported from configs\n",
      "sde ready\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'ncsnpp'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m scaler \u001b[38;5;241m=\u001b[39m get_data_scaler(config)\n\u001b[1;32m     32\u001b[0m inverse_scaler \u001b[38;5;241m=\u001b[39m get_data_inverse_scaler(config)\n\u001b[0;32m---> 33\u001b[0m score_model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore model ready\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m get_optimizer(config, score_model\u001b[38;5;241m.\u001b[39mparameters())\n",
      "File \u001b[0;32m~/score-face/nbs/../models/utils.py:91\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m\"\"\"Create the score model.\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m model_name \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m---> 91\u001b[0m score_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m(config)\n\u001b[1;32m     92\u001b[0m score_model \u001b[38;5;241m=\u001b[39m score_model\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     93\u001b[0m score_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mDataParallel(score_model)\n",
      "File \u001b[0;32m~/score-face/nbs/../models/utils.py:47\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m(name):\n\u001b[0;32m---> 47\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MODELS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ncsnpp'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "obj_path = \"/cluster/scratch/aarslan/FFHQ/mesh_and_texture/20705.obj\"\n",
    "verts, faces, aux = load_obj(obj_path, device=device)\n",
    "verts_uvs = aux.verts_uvs[None, ...]  # (1, V, 2)\n",
    "faces_uvs = faces.textures_idx[None, ...]  # (1, F, 3)\n",
    "tex_maps = aux.texture_images\n",
    "texture_image = list(tex_maps.values())[0][None, ...].to(device).permute(0, 3, 1, 2)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_random_elev_azimuth():\n",
    "    elev = (np.random.random() * 30) - 60\n",
    "    azimuth = (np.random.random() * 30) - 60\n",
    "    return elev, azimuth\n",
    "\n",
    "\n",
    "def get_grad_texture(texture, elev, azimuth, grad_face, face, renderer):\n",
    "    render_partial = partial(renderer.render, elev=elev, azimuth=azimuth)\n",
    "    new_face, grad_texture = torch.autograd.functional.vjp(func=render_partial, inputs=texture,\n",
    "                                                           v=grad_face, create_graph=False, strict=False)\n",
    "    return grad_texture\n",
    "\n",
    "\n",
    "def sample_texture():\n",
    "    snr = 0.075\t #@param {\"type\": \"number\"}\n",
    "    n_steps = 1 #@param {\"type\": \"integer\"}\n",
    "    probability_flow = False #@param {\"type\": \"boolean\"}\n",
    "    continuous = config.training.continuous\n",
    "    eps = sampling_eps\n",
    "    device = config.device\n",
    "    denoise = True\n",
    "    renderer = Renderer()\n",
    "\n",
    "    # Initial sample\n",
    "    texture = torch.randn(size=(cfg[\"batch_size\"], cfg[\"texture_num_channels\"], cfg[\"texture_height\"], cfg[\"texture_width\"]), device=device, requires_grad=True)\n",
    "    elev, azimuth = get_random_elev_azimuth()\n",
    "    face = renderer.render(texture=texture, elev=elev, azimuth=azimuth).detach() # (N, C, H, W)\n",
    "\n",
    "    timesteps = torch.linspace(sde.T, eps, sde.N, device=device)\n",
    "    score_fn = get_score_fn(sde, score_model, train=False, continuous=continuous)\n",
    "    rsde = sde.reverse(score_fn, probability_flow)\n",
    "\n",
    "    for i in range(sde.N):\n",
    "        t = timesteps[i]\n",
    "        vec_t = torch.ones(cfg[\"batch_size\"], device=t.device) * t\n",
    "        alpha = torch.ones_like(vec_t)\n",
    "\n",
    "        # langevin corrector\n",
    "        for j in range(n_steps):\n",
    "            grad_face = score_fn(face, vec_t) # (N, C, H, W)\n",
    "            grad_texture = get_grad_texture(texture=texture, elev=elev, azimuth=azimuth, grad_face=grad_face, face=face, renderer=renderer)\n",
    "            noise_texture = torch.randn_like(texture)\n",
    "            grad_norm = torch.norm(grad_texture.reshape(grad_texture.shape[0], -1), dim=-1).mean()\n",
    "            noise_norm = torch.norm(noise_texture.reshape(noise_texture.shape[0], -1), dim=-1).mean()\n",
    "            step_size = (snr * noise_norm / grad_norm) ** 2 * 2 * alpha\n",
    "            texture_mean = texture + step_size[:, None, None, None] * grad_texture\n",
    "            texture = (texture_mean + torch.sqrt(step_size * 2)[:, None, None, None] * noise_texture).detach()\n",
    "            texture.requires_grad = True\n",
    "            elev, azimuth = get_random_elev_azimuth()\n",
    "            face = renderer.render(texture=texture, elev=elev, azimuth=azimuth).detach()\n",
    "        \n",
    "        # reverse diffusion predictor\n",
    "        f, G = rsde.discretize(face, vec_t)\n",
    "        z = torch.randn_like(face)\n",
    "        face_mean = face - f\n",
    "        face = face_mean + G[:, None, None, None] * z\n",
    "\n",
    "    face = inverse_scaler(face_mean if denoise else face)\n",
    "    texture = inverse_scaler(texture_mean if denoise else texture)\n",
    "    print(\"Number of function evaluations:\", sde.N * (n_steps + 1))\n",
    "\n",
    "    print(\"Showing last face...\")\n",
    "    show_samples(face)\n",
    "\n",
    "    print(\"Showing last texture...\")\n",
    "    show_samples(texture)\n",
    "\n",
    "    print(\"Showing new faces...\")\n",
    "    show_samples(renderer.render(texture))\n",
    "    show_samples(renderer.render(texture))\n",
    "    show_samples(renderer.render(texture))\n",
    "\n",
    "sample_texture()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device {device}.\")\n",
    "from pytorch3d.io import load_obj\n",
    "from pytorch3d.renderer.mesh import Textures\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    FoVPerspectiveCameras, \n",
    "    PointLights, \n",
    "    Materials, \n",
    "    RasterizationSettings, \n",
    "    MeshRenderer, \n",
    "    MeshRasterizer,  \n",
    "    SoftPhongShader\n",
    ")\n",
    "\n",
    "obj_path = \"/cluster/scratch/aarslan/FFHQ/mesh_and_texture/20705.obj\"\n",
    "verts, faces, aux = load_obj(obj_path, device=device)\n",
    "verts_uvs = aux.verts_uvs[None, ...]  # (1, V, 2)\n",
    "faces_uvs = faces.textures_idx[None, ...]  # (1, F, 3)\n",
    "tex_maps = aux.texture_images\n",
    "texture_image = list(tex_maps.values())[0][None, ...].to(device)\n",
    "# texture_image = torch.randn_like(texture_image, device=device, requires_grad=True)\n",
    "print(texture_image.shape)\n",
    "\n",
    "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
    "# 512x512. As we are rendering images for visualization purposes only we will set faces_per_pixel=1\n",
    "# and blur_radius=0.0. We also set bin_size and max_faces_per_bin to None which ensure that \n",
    "# the faster coarse-to-fine rasterization method is used. Refer to rasterize_meshes.py for \n",
    "# explanations of these parameters. Refer to docs/notes/renderer.md for an explanation of \n",
    "# the difference between naive and coarse-to-fine rasterization. \n",
    "\n",
    "tex = Textures(verts_uvs=verts_uvs, faces_uvs=faces_uvs, maps=texture_image)\n",
    "meshes = Meshes(verts=[verts], faces=[faces.verts_idx], textures=tex)\n",
    "\n",
    "# We scale normalize and center the target mesh to fit in a sphere of radius 1 \n",
    "# centered at (0,0,0). (scale, center) will be used to bring the predicted mesh \n",
    "# to its original center and scale.  Note that normalizing the target mesh, \n",
    "# speeds up the optimization but is not necessary!\n",
    "verts_packed = meshes.verts_packed()\n",
    "N = verts_packed.shape[0]\n",
    "center = verts_packed.mean(0)\n",
    "scale = max((verts_packed - center).abs().max(0)[0])\n",
    "meshes.offset_verts_(-center)\n",
    "meshes.scale_verts_((1.0 / float(scale)))\n",
    "\n",
    "# Randomize dist, elev and azim parameters.\n",
    "# With world coordinates +Y up, +X left and +Z in.\n",
    "R, T = look_at_view_transform(2.0, 0, 0)\n",
    "cameras = FoVPerspectiveCameras(R=R, T=T, znear=1, zfar=10000, fov=40, degrees=True, device=device)\n",
    "\n",
    "# dist=2, elev=0, azimuth=180, fov=40, image_size=256, R=None, T=None\n",
    "\n",
    "# Place a point light in front of the object.\n",
    "lights = PointLights(diffuse_color=((0.7, 0.7, 0.7),), ambient_color=((0.2, 0.2, 0.2),), device=device,\n",
    "                     location=T) # ambient_color=((0.2, 0.2, 0.2),), \n",
    "\n",
    "# Create a Phong renderer by composing a rasterizer and a shader. The textured Phong shader will \n",
    "# interpolate the texture uv coordinates for each vertex, sample from a texture image and \n",
    "# apply the Phong lighting model\n",
    "renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftPhongShader(\n",
    "        device=device, \n",
    "        cameras=cameras,\n",
    "        lights=lights\n",
    "    )\n",
    ")\n",
    "\n",
    "images = renderer(meshes)[:, :, :, :3].permute(0, 3, 1, 2)  # (N, C, H, W)\n",
    "display(Image.fromarray((torch.clamp(images[0], 0, 1).permute(1, 2, 0).detach().cpu().numpy() * 255).astype(np.uint8)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sample_face():\n",
    "    img_size = config.data.image_size\n",
    "    channels = config.data.num_channels\n",
    "    shape = (batch_size, channels, img_size, img_size)\n",
    "    snr = 0.075\t #@param {\"type\": \"number\"}\n",
    "    n_steps = 1 #@param {\"type\": \"integer\"}\n",
    "    probability_flow = False #@param {\"type\": \"boolean\"}\n",
    "    continuous = config.training.continuous\n",
    "    eps = sampling_eps\n",
    "    device = config.device\n",
    "    denoise = True\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Initial sample\n",
    "        face = sde.prior_sampling(shape).to(device)\n",
    "        timesteps = torch.linspace(sde.T, eps, sde.N, device=device)\n",
    "        score_fn = get_score_fn(sde, score_model, train=False, continuous=continuous)\n",
    "        rsde = sde.reverse(score_fn, probability_flow)\n",
    "\n",
    "        for i in range(sde.N):\n",
    "            t = timesteps[i]\n",
    "            vec_t = torch.ones(shape[0], device=t.device) * t\n",
    "            alpha = torch.ones_like(vec_t)\n",
    "\n",
    "            # langevin corrector\n",
    "            for i in range(n_steps):\n",
    "                grad = score_fn(face, vec_t)\n",
    "                noise = torch.randn_like(face)\n",
    "                grad_norm = torch.norm(grad.reshape(grad.shape[0], -1), dim=-1).mean()\n",
    "                noise_norm = torch.norm(noise.reshape(noise.shape[0], -1), dim=-1).mean()\n",
    "                step_size = (snr * noise_norm / grad_norm) ** 2 * 2 * alpha\n",
    "                face_mean = face + step_size[:, None, None, None] * grad\n",
    "                face = face_mean + torch.sqrt(step_size * 2)[:, None, None, None] * noise\n",
    "            \n",
    "            # reverse diffusion predictor\n",
    "            f, G = rsde.discretize(face, vec_t)\n",
    "            z = torch.randn_like(face)\n",
    "            face_mean = face - f\n",
    "            face = face_mean + G[:, None, None, None] * z\n",
    "\n",
    "        face = inverse_scaler(face_mean if denoise else face)\n",
    "        print(\"Number of function evaluations:\", sde.N * (n_steps + 1))\n",
    "\n",
    "    show_samples(face)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample_face()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device {device}.\")\n",
    "from pytorch3d.io import load_obj\n",
    "from pytorch3d.renderer.mesh import Textures\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    FoVPerspectiveCameras, \n",
    "    PointLights, \n",
    "    Materials, \n",
    "    RasterizationSettings, \n",
    "    MeshRenderer, \n",
    "    MeshRasterizer,  \n",
    "    SoftPhongShader\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import datasets\n",
    "import controllable_generation\n",
    "from sde_lib import VESDE\n",
    "from sampling import ReverseDiffusionPredictor, LangevinCorrector\n",
    "from configs.ve import ffhq_256_ncsnpp_continuous as configs\n",
    "from utils import restore_checkpoint\n",
    "from models import utils as mutils\n",
    "from models import ncsnpp\n",
    "from models.ema import ExponentialMovingAverage\n",
    "from losses import get_optimizer\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "obj_path = \"/cluster/scratch/aarslan/FFHQ/mesh_and_texture/03088.obj\"\n",
    "verts, faces, aux = load_obj(obj_path, device=device)\n",
    "verts_uvs = aux.verts_uvs[None, ...]  # (1, V, 2)\n",
    "faces_uvs = faces.textures_idx[None, ...]  # (1, F, 3)\n",
    "tex_maps = aux.texture_images\n",
    "texture_image = list(tex_maps.values())[0]\n",
    "texture_image = torch.randn_like(texture_image, device=device)[None, ...]\n",
    "\n",
    "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
    "# 512x512. As we are rendering images for visualization purposes only we will set faces_per_pixel=1\n",
    "# and blur_radius=0.0. We also set bin_size and max_faces_per_bin to None which ensure that \n",
    "# the faster coarse-to-fine rasterization method is used. Refer to rasterize_meshes.py for \n",
    "# explanations of these parameters. Refer to docs/notes/renderer.md for an explanation of \n",
    "# the difference between naive and coarse-to-fine rasterization. \n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=256, # 1024\n",
    "    blur_radius=0.0,\n",
    "    faces_per_pixel=5\n",
    ")\n",
    "\n",
    "# Place a point light in front of the object.\n",
    "lights = PointLights(device=device, location=[[0.0, 0.0, 0.0]])\n",
    "\n",
    "# for loop\n",
    "\n",
    "tex = Textures(verts_uvs=verts_uvs, faces_uvs=faces_uvs, maps=texture_image)\n",
    "meshes = Meshes(verts=[verts], faces=[faces.verts_idx], textures=tex)\n",
    "\n",
    "# We scale normalize and center the target mesh to fit in a sphere of radius 1 \n",
    "# centered at (0,0,0). (scale, center) will be used to bring the predicted mesh \n",
    "# to its original center and scale.  Note that normalizing the target mesh, \n",
    "# speeds up the optimization but is not necessary!\n",
    "verts_packed = meshes.verts_packed()\n",
    "N = verts_packed.shape[0]\n",
    "center = verts_packed.mean(0)\n",
    "scale = max((verts_packed - center).abs().max(0)[0])\n",
    "meshes.offset_verts_(-center)\n",
    "meshes.scale_verts_((1.0 / float(scale)))\n",
    "\n",
    "# Randomize dist, elev and azim parameters.\n",
    "# With world coordinates +Y up, +X left and +Z in.\n",
    "R, T = look_at_view_transform(dist=2.0, elev=0, azim=0.0)\n",
    "cameras = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
    "\n",
    "# Create a Phong renderer by composing a rasterizer and a shader. The textured Phong shader will \n",
    "# interpolate the texture uv coordinates for each vertex, sample from a texture image and \n",
    "# apply the Phong lighting model\n",
    "renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftPhongShader(\n",
    "        device=device, \n",
    "        cameras=cameras,\n",
    "        lights=lights\n",
    "    )\n",
    ")\n",
    "\n",
    "images = renderer(meshes)[:, :, :, :3].permute(0, 3, 1, 2)  # (N, C, H, W)\n",
    "display(Image.fromarray((images[0].permute(1, 2, 0).detach().cpu().numpy() * 255).astype(np.uint8)))\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img_size = config.data.image_size\n",
    "channels = config.data.num_channels\n",
    "shape = (batch_size, channels, img_size, img_size)\n",
    "predictor = ReverseDiffusionPredictor #@param [\"EulerMaruyamaPredictor\", \"AncestralSamplingPredictor\", \"ReverseDiffusionPredictor\", \"None\"] {\"type\": \"raw\"}\n",
    "corrector = LangevinCorrector #@param [\"LangevinCorrector\", \"AnnealedLangevinDynamics\", \"None\"] {\"type\": \"raw\"}\n",
    "snr = 0.16 #@param {\"type\": \"number\"}\n",
    "n_steps =  1#@param {\"type\": \"integer\"}\n",
    "probability_flow = False #@param {\"type\": \"boolean\"}\n",
    "sampling_fn = sampling.get_pc_sampler(sde, shape, predictor, corrector,\n",
    "                                      inverse_scaler, snr, n_steps=n_steps,\n",
    "                                      probability_flow=probability_flow,\n",
    "                                      continuous=config.training.continuous,\n",
    "                                      eps=sampling_eps, device=config.device)\n",
    "\n",
    "x, n = sampling_fn(score_model)\n",
    "show_samples(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "This code is from pl_bolts.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper: `U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "    <https://arxiv.org/abs/1505.04597>`_\n",
    "    Paper authors: Olaf Ronneberger, Philipp Fischer, Thomas Brox\n",
    "    Implemented by:\n",
    "        - `Annika Brundyn <https://github.com/annikabrundyn>`_\n",
    "        - `Akshay Kulkarni <https://github.com/akshaykvnit>`_\n",
    "    Args:\n",
    "        output_nc: Number of output classes required\n",
    "        input_channels: Number of channels in input images (default 3)\n",
    "        num_layers: Number of layers in each side of U-net (default 5)\n",
    "        features_start: Number of features in first layer (default 64)\n",
    "        bilinear: Whether to use bilinear interpolation or transposed convolutions (default) for upsampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            output_nc: int,\n",
    "            input_channels: int = 3,\n",
    "            num_layers: int = 5,\n",
    "            features_start: int = 64,\n",
    "            bilinear: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        layers = [DoubleConv(input_channels, features_start)]\n",
    "\n",
    "        feats = features_start\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(Down(feats, feats * 2))\n",
    "            feats *= 2\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(Up(feats, feats // 2, bilinear))\n",
    "            feats //= 2\n",
    "\n",
    "        layers.append(nn.Conv2d(feats, output_nc, kernel_size=1))\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xi = [self.layers[0](x)]\n",
    "        # Down path\n",
    "        for layer in self.layers[1:self.num_layers]:\n",
    "            xi.append(layer(xi[-1]))\n",
    "        # Up path\n",
    "        for i, layer in enumerate(self.layers[self.num_layers:-1]):\n",
    "            xi[-1] = layer(xi[-1], xi[-2 - i])\n",
    "        x_out = self.layers[-1](xi[-1])\n",
    "        return x_out\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    [ Conv2d => BatchNorm (optional) => ReLU ] x 2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"\n",
    "    Downscale with MaxPool => DoubleConvolution block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            DoubleConv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling (by either bilinear interpolation or transpose convolutions)\n",
    "    followed by concatenation of feature map from contracting path, followed by DoubleConv.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch: int, out_ch: int, bilinear: bool = False):\n",
    "        super().__init__()\n",
    "        self.upsample = None\n",
    "        if bilinear:\n",
    "            self.upsample = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True),\n",
    "                nn.Conv2d(in_ch, in_ch // 2, kernel_size=1),\n",
    "            )\n",
    "        else:\n",
    "            self.upsample = nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upsample(x1)\n",
    "\n",
    "        # Pad x1 to the size of x2\n",
    "        diff_h = x2.shape[2] - x1.shape[2]\n",
    "        diff_w = x2.shape[3] - x1.shape[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diff_w // 2, diff_w - diff_w // 2, diff_h // 2, diff_h - diff_h // 2])\n",
    "\n",
    "        # Concatenate along the channels axis\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch.nn as nn\n",
    "from pl_bolts.models.autoencoders.components import (\n",
    "    resize_conv1x1, resize_conv3x3\n",
    ")\n",
    "\n",
    "\n",
    "class ConditionedDecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet block, but convs replaced with resize convs, and channel increase is in\n",
    "    second conv, not first.\n",
    "    Also heavily borrowed from pl_bolts.models.autoencoders.components.\n",
    "    \"\"\"\n",
    "\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, scale=1, upsample=None, condition_dim=16):\n",
    "        super().__init__()\n",
    "        self.conv1 = resize_conv3x3(inplanes + condition_dim,\n",
    "                                    inplanes)  # 2 is the feature dimension for the conditioning\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = resize_conv3x3(inplanes, planes, scale)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.upsample = upsample\n",
    "\n",
    "        self.interpolation_mode = \"bilinear\"\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data[\"features\"]\n",
    "        condition = data[\"condition\"]\n",
    "\n",
    "        condition_scaled = nn.functional.interpolate(condition, x.shape[-2:], mode=self.interpolation_mode)\n",
    "        identity = x\n",
    "\n",
    "        out = torch.cat([x, condition_scaled], 1)  # Along the channel dimension\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.upsample is not None:\n",
    "            identity = self.upsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out_dict = {\"features\": out, \"condition\": condition}\n",
    "        return out_dict\n",
    "\n",
    "\n",
    "class EarlyConditionedSimpleResNetDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified from pl_bolts.models.autoencoders.components.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block, layers, latent_dim, input_height, nc_texture, first_conv=False, maxpool1=False,\n",
    "                 condition_dim=16, **kwargs):\n",
    "        # TODO: refactor duplicated code\n",
    "        super().__init__()\n",
    "\n",
    "        self.condition_dim = condition_dim\n",
    "\n",
    "        self.expansion = block.expansion\n",
    "        self.inplanes = 512 * block.expansion\n",
    "        self.first_conv = first_conv\n",
    "        self.maxpool1 = maxpool1\n",
    "        self.input_height = input_height\n",
    "\n",
    "        self.linear = nn.Linear(latent_dim, self.inplanes)\n",
    "\n",
    "        self.initial = self._make_layer(block, 512, layers[0], scale=2, condition_dim=self.condition_dim)\n",
    "\n",
    "        self.layer0 = self._make_layer(block, 256, layers[0], scale=2, condition_dim=self.condition_dim)\n",
    "        self.layer1 = self._make_layer(block, 256, layers[0], scale=2, condition_dim=self.condition_dim)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], scale=2, condition_dim=self.condition_dim)\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], scale=2, condition_dim=self.condition_dim)\n",
    "\n",
    "        if self.input_height == 128:\n",
    "            self.layer4 = self._make_layer(block, 64, layers[3], condition_dim=self.condition_dim)\n",
    "        elif self.input_height == 256:\n",
    "            self.layer4 = self._make_layer(block, 64, layers[3], scale=2, condition_dim=self.condition_dim)\n",
    "        else:\n",
    "            raise Warning(\"Invalid input height: '{}\".format(self.input_height))\n",
    "\n",
    "        if self.first_conv:\n",
    "            self.upscale = Interpolate(scale_factor=2)\n",
    "            self.upscale_factor *= 2\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            64 * block.expansion, nc_texture, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, scale=1, condition_dim=16):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            block:\n",
    "            planes: int number of channels\n",
    "            blocks: int number of blocks (e.g. 2)\n",
    "            scale:\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        upsample = None\n",
    "        if scale != 1 or self.inplanes != planes * block.expansion:\n",
    "            upsample = nn.Sequential(\n",
    "                resize_conv1x1(self.inplanes, planes * block.expansion, scale),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, scale, upsample, condition_dim=condition_dim))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, condition_dim=condition_dim))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        x = self.linear(x)\n",
    "        # We now have 512 feature maps with the same value in all spatial locations\n",
    "        # self.inplanes changes when creating blocks\n",
    "        x = x.view(x.shape[0], 512, 1, 1).expand(-1, -1, 4, 4)\n",
    "\n",
    "        in_dict = {\"features\": x, \"condition\": condition}\n",
    "        out_dict = self.initial(in_dict)\n",
    "\n",
    "        out_dict = self.layer0(out_dict)\n",
    "        out_dict = self.layer1(out_dict)\n",
    "        out_dict = self.layer2(out_dict)\n",
    "        out_dict = self.layer3(out_dict)\n",
    "        out_dict = self.layer4(out_dict)\n",
    "        x = out_dict['features']\n",
    "        x = self.conv1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AdditiveDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Additive decoder. Produces the additive feature image.\n",
    "    Adapted from https://github.com/mcbuehler/VariTex/blob/385745548ddb0b770b9bc3f4c8716402fe0e245a/varitex/modules/decoder.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        latent_dim_additive = cfg[\"latent_dim_additive\"]\n",
    "        input_height = cfg[\"image_height\"]\n",
    "        out_dim = cfg[\"num_channels\"]\n",
    "        condition_dim = cfg[\"num_channels\"]  # We condition on a neural texture for the face interior\n",
    "        # Resnet-18 variant\n",
    "        self.decoder = EarlyConditionedSimpleResNetDecoder(ConditionedDecoderBlock, [2, 2, 2, 2], latent_dim_additive,\n",
    "                                                            input_height, nc_texture=out_dim,\n",
    "                                                            condition_dim=condition_dim)\n",
    "\n",
    "    def forward(self, rendered_images, latent_vectors):\n",
    "        \"\"\"\n",
    "            Inputs:\n",
    "                rendered_images: (N, cfg[\"num_channels\"], H, W)\n",
    "                latent_vectors: (N, latent_dim_additive)\n",
    "            \n",
    "            Output:\n",
    "                additive_feature_images: (N, cfg[\"num_channels\"], H, W)\n",
    "        \"\"\"\n",
    "\n",
    "        # The sampled texture should already have been masked to the face interior\n",
    "        # Errors should not back-propagate through the sampled face interior texture\n",
    "        additive_feature_images = self.decoder(latent_vectors, rendered_images.detach())\n",
    "        return additive_feature_images\n",
    "\n",
    "additive_decoder = AdditiveDecoder(cfg).to(device)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Feature2ImageRenderer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.unet = UNet(output_nc=cfg[\"num_channels\"], input_channels=cfg[\"num_channels\"] * 2, features_start=cfg[\"nc_feature2image\"],\n",
    "                         num_layers=cfg[\"feature2image_num_layers\"])\n",
    "\n",
    "    def forward(self, concatted_images):\n",
    "        return self.unet(concatted_images)\n",
    "\n",
    "feature_to_image_renderer = Feature2ImageRenderer(cfg).to(device)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "latent_vectors = torch.randn(size=(cfg[\"batch_size\"], cfg[\"latent_dim_additive\"]), device=device)\n",
    "additive_feature_images = additive_decoder(images, latent_vectors)\n",
    "concatted_images = torch.concat([images, additive_feature_images], axis=1)\n",
    "filled_images = feature_to_image_renderer(concatted_images)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Image.fromarray((filled_images[0].permute(1, 2, 0).detach().cpu().numpy() * 255).astype(np.uint8))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ckpt_filename = \"../exp/ve/ffhq_256_ncsnpp_continuous/checkpoint_48.pth\"\n",
    "config = configs.get_config()\n",
    "sde = VESDE(sigma_min=config.model.sigma_min, sigma_max=config.model.sigma_max, N=config.model.num_scales)\n",
    "sampling_eps = 1e-5\n",
    "\n",
    "batch_size = 1 #@param {\"type\":\"integer\"}\n",
    "config.training.batch_size = batch_size\n",
    "config.eval.batch_size = batch_size\n",
    "\n",
    "random_seed = 0 #@param {\"type\": \"integer\"}\n",
    "\n",
    "sigmas = mutils.get_sigmas(config)\n",
    "scaler = datasets.get_data_scaler(config)\n",
    "inverse_scaler = datasets.get_data_inverse_scaler(config)\n",
    "score_model = mutils.create_model(config)\n",
    "\n",
    "optimizer = get_optimizer(config, score_model.parameters())\n",
    "ema = ExponentialMovingAverage(score_model.parameters(),\n",
    "                               decay=config.model.ema_rate)\n",
    "state = dict(step=0, optimizer=optimizer,\n",
    "             model=score_model, ema=ema)\n",
    "\n",
    "state = restore_checkpoint(ckpt_filename, state, config.device)\n",
    "ema.copy_to(score_model.parameters())\n",
    "\n",
    "def image_grid(x):\n",
    "    size = config.data.image_size\n",
    "    channels = config.data.num_channels\n",
    "    img = x.reshape(-1, size, size, channels)\n",
    "    w = int(np.sqrt(img.shape[0]))\n",
    "    img = img.reshape((w, w, size, size, channels)).transpose((0, 2, 1, 3, 4)).reshape((w * size, w * size, channels))\n",
    "    return img\n",
    "\n",
    "\n",
    "def show_samples(x):\n",
    "    x = x.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
    "    img = image_grid(x)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "predictor = ReverseDiffusionPredictor #@param [\"EulerMaruyamaPredictor\", \"AncestralSamplingPredictor\", \"ReverseDiffusionPredictor\", \"None\"] {\"type\": \"raw\"}\n",
    "corrector = LangevinCorrector #@param [\"LangevinCorrector\", \"AnnealedLangevinDynamics\", \"None\"] {\"type\": \"raw\"}\n",
    "snr = 0.075 #@param {\"type\": \"number\"}\n",
    "n_steps = 1 #@param {\"type\": \"integer\"}\n",
    "probability_flow = False #@param {\"type\": \"boolean\"}\n",
    "inverse_scaler = datasets.get_data_inverse_scaler(config)\n",
    "\n",
    "pc_inpainter = controllable_generation.get_pc_inpainter(sde,\n",
    "                                                        predictor,\n",
    "                                                        corrector,\n",
    "                                                        inverse_scaler,\n",
    "                                                        snr=snr,\n",
    "                                                        n_steps=n_steps,\n",
    "                                                        probability_flow=probability_flow,\n",
    "                                                        continuous=config.training.continuous,\n",
    "                                                        denoise=True)\n",
    "\n",
    "show_samples(images)\n",
    "show_samples(images * masks)\n",
    "inpainted_images = pc_inpainter(score_model, scaler(images), masks)\n",
    "show_samples(inpainted_images)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.9 64-bit ('score_face': conda)"
  },
  "interpreter": {
   "hash": "5039bcb0086f1a8e3ceb7068972048611f48699149712ad1b3124db6e7ca405f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}